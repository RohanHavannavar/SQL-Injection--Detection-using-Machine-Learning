{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.8 Featurizing text data or Given SQL Queries </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing essential libararies\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import hstack\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the preprocessed data\n",
    "data = pd.read_csv('feature_extracted_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Unnamed: 0',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Query', 'Label', 'query_len', 'num_words_query', 'no_single_qts',\n",
       "       'no_double_qts', 'no_punct', 'no_single_cmnt', 'no_mult_cmnt',\n",
       "       'no_space', 'no_perc', 'no_log_opt', 'no_arith', 'no_null', 'no_hexa',\n",
       "       'no_alpha', 'no_digit', 'len_of_chr_char_null', 'genuine_keywords',\n",
       "       'log_query_len', 'box_query_len'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Label</th>\n",
       "      <th>query_len</th>\n",
       "      <th>num_words_query</th>\n",
       "      <th>no_single_qts</th>\n",
       "      <th>no_double_qts</th>\n",
       "      <th>no_punct</th>\n",
       "      <th>no_single_cmnt</th>\n",
       "      <th>no_mult_cmnt</th>\n",
       "      <th>no_space</th>\n",
       "      <th>...</th>\n",
       "      <th>no_log_opt</th>\n",
       "      <th>no_arith</th>\n",
       "      <th>no_null</th>\n",
       "      <th>no_hexa</th>\n",
       "      <th>no_alpha</th>\n",
       "      <th>no_digit</th>\n",
       "      <th>len_of_chr_char_null</th>\n",
       "      <th>genuine_keywords</th>\n",
       "      <th>log_query_len</th>\n",
       "      <th>box_query_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" or pg_sleep  (  __time__  )  --</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.496508</td>\n",
       "      <td>4.571114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>create user name identified by pass123 tempora...</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.499810</td>\n",
       "      <td>6.378650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and 1  =  utl_inaddr.get_host_address   (    ...</td>\n",
       "      <td>1</td>\n",
       "      <td>218</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.384495</td>\n",
       "      <td>8.209386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>select * from users where id  =  '1' or @ @1 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.499810</td>\n",
       "      <td>6.378650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>select * from users where id  =  1 or 1#\"  ( ...</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.442651</td>\n",
       "      <td>6.268357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  Label  query_len  \\\n",
       "0                  \" or pg_sleep  (  __time__  )  --      1         33   \n",
       "1  create user name identified by pass123 tempora...      1         90   \n",
       "2   and 1  =  utl_inaddr.get_host_address   (    ...      1        218   \n",
       "3   select * from users where id  =  '1' or @ @1 ...      1         90   \n",
       "4   select * from users where id  =  1 or 1#\"  ( ...      1         85   \n",
       "\n",
       "   num_words_query  no_single_qts  no_double_qts  no_punct  no_single_cmnt  \\\n",
       "0                7              0              1        10               1   \n",
       "1               12              0              0         1               0   \n",
       "2               35              3              0        25               0   \n",
       "3               20              3              0        13               1   \n",
       "4               18              0              1        10               1   \n",
       "\n",
       "   no_mult_cmnt  no_space  ...  no_log_opt  no_arith  no_null  no_hexa  \\\n",
       "0             0         6  ...           1         2        0        0   \n",
       "1             0        11  ...           0         0        0        0   \n",
       "2             0        35  ...           2         0        0        0   \n",
       "3             0        20  ...           1         3        0        0   \n",
       "4             0        18  ...           1         3        0        0   \n",
       "\n",
       "   no_alpha  no_digit  len_of_chr_char_null  genuine_keywords  log_query_len  \\\n",
       "0        13         0                     0                 0       3.496508   \n",
       "1        75         3                     0                 0       4.499810   \n",
       "2       120         2                     0                 2       5.384495   \n",
       "3        42         5                     0                 2       4.499810   \n",
       "4        42         4                     0                 2       4.442651   \n",
       "\n",
       "   box_query_len  \n",
       "0       4.571114  \n",
       "1       6.378650  \n",
       "2       8.209386  \n",
       "3       6.378650  \n",
       "4       6.268357  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the query_len,num_words_query because from the EDA we got to know that it is not very much helpful in predicting the output classes\n",
    "data.drop(['query_len','num_words_query'],axis= 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = data['Label']\n",
    "x = data.drop('Label',axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.8.1 Splitting the dataset to train and test </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size is (21523, 18)\n",
      "test dataset size is (9225, 18)\n"
     ]
    }
   ],
   "source": [
    "#will split the datset in 70 and 30 i.e 70 percent for training and 30 percent for test\n",
    "#since the class is imbalanced will do stratify sampling\n",
    "\n",
    "#will remove the target variable i.e label column from datset\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,stratify = y)\n",
    "\n",
    "\n",
    "print(\"train dataset size is {}\".format(x_train.shape))\n",
    "print(\"test dataset size is {}\".format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing for future use\n",
    "with open('x_train','wb') as f:\n",
    "    pickle.dump(x_train,f)\n",
    "with open('x_test','wb') as f:\n",
    "    pickle.dump(x_test,f)\n",
    "with open('y_train','wb') as f:\n",
    "    pickle.dump(y_train,f)\n",
    "with open('y_test','wb') as f:\n",
    "    pickle.dump(y_test,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2> 3.8.2 Featurizing text data or Given SQL Queries using Bag of words </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Encoding text data\n",
      "the shape of train dataset unigram is (21523, 18629)\n",
      "the shape of test dataset unigram is  (9225, 18629)\n"
     ]
    }
   ],
   "source": [
    "#using countvectorizer for bag of words\n",
    "#using bag of words for unigram range\n",
    "\n",
    "unigram_bow = CountVectorizer(ngram_range = (1,1))\n",
    "train_bow = unigram_bow.fit(x_train['Query'].values)\n",
    "\n",
    "\n",
    "print(\"After Encoding text data\")\n",
    "x_train_bow_unigram = train_bow.transform(x_train['Query'].values)\n",
    "x_test_bow_unigram = train_bow.transform(x_test['Query'].values)\n",
    "\n",
    "\n",
    "print(\"the shape of train dataset unigram is {}\".format(x_train_bow_unigram.shape))\n",
    "print(\"the shape of test dataset unigram is  {}\".format(x_test_bow_unigram.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unigram_bow','wb') as f:\n",
    "    pickle.dump(unigram_bow,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving train_bow for future use\n",
    "with open('train_bow','wb') as f:\n",
    "    pickle.dump(train_bow,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Encoding text data\n",
      "the shape of train dataset unigram is (21523, 51979)\n",
      "the shape of test dataset unigram is  (9225, 51979)\n"
     ]
    }
   ],
   "source": [
    "#will use bigram range of values to build bow\n",
    "bigram_bow = CountVectorizer(ngram_range = (2,2))\n",
    "train_bigram_bow = bigram_bow.fit(x_train['Query'].values)\n",
    "\n",
    "print(\"After Encoding text data\")\n",
    "x_train_bow_bigram = train_bigram_bow.transform(x_train['Query'].values)\n",
    "x_test_bow_bigram = train_bigram_bow.transform(x_test['Query'].values)\n",
    "\n",
    "\n",
    "print(\"the shape of train dataset unigram is {}\".format(x_train_bow_bigram.shape))\n",
    "print(\"the shape of test dataset unigram is  {}\".format(x_test_bow_bigram.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.8.3 Featurizing text data or Given SQL Queries using tfidf vectorizer </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Encoding text data\n",
      "the shape of train dataset unigram is (21523, 18629)\n",
      "the shape of test dataset unigram is  (9225, 18629)\n"
     ]
    }
   ],
   "source": [
    "#considering only unigrams\n",
    "tfidf_bow = TfidfVectorizer(ngram_range = (1,1))\n",
    "tfidf_train_bow = tfidf_bow.fit(x_train['Query'].values)\n",
    "\n",
    "\n",
    "print(\"After Encoding text data\")\n",
    "x_train_tfidf_unigram = tfidf_train_bow.transform(x_train['Query'].values)\n",
    "x_test_tfidf_unigram = tfidf_train_bow.transform(x_test['Query'].values)\n",
    "\n",
    "\n",
    "print(\"the shape of train dataset unigram is {}\".format(x_train_tfidf_unigram.shape))\n",
    "print(\"the shape of test dataset unigram is  {}\".format(x_test_tfidf_unigram.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Encoding text data\n",
      "the shape of train dataset unigram is (21523, 51979)\n",
      "the shape of test dataset unigram is  (9225, 51979)\n"
     ]
    }
   ],
   "source": [
    "#considering only bigrams\n",
    "tfidf = TfidfVectorizer(ngram_range = (2,2))\n",
    "tfidf_train_bigram = tfidf.fit(x_train['Query'].values)\n",
    "\n",
    "\n",
    "print(\"After Encoding text data\")\n",
    "x_train_tfidf_bigram = tfidf_train_bigram.transform(x_train['Query'].values)\n",
    "x_test_tfidf_bigram = tfidf_train_bigram.transform(x_test['Query'].values)\n",
    "\n",
    "\n",
    "print(\"the shape of train dataset unigram is {}\".format(x_train_tfidf_bigram.shape))\n",
    "print(\"the shape of test dataset unigram is  {}\".format(x_test_tfidf_bigram.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.8.4 Featurizing text data or Given SQL Queries using Average word2vec </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.8.4.1 Using pretrained Glove vectors for words Embeddings </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading glove vector file\n",
    "with open('glove_vectors', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words =  set(model.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21523/21523 [00:00<00:00, 56886.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21523\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#compute average word2vec for each train data query\n",
    "avg_w2v_vectors_train = [] \n",
    "for sentence in tqdm(x_train['Query'].values): \n",
    "    vector = np.zeros(300) \n",
    "    cnt_words =0\n",
    "    for word in sentence.split(): \n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_train.append(vector)\n",
    "\n",
    "print(len(avg_w2v_vectors_train))\n",
    "print(len(avg_w2v_vectors_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9225/9225 [00:00<00:00, 58992.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9225\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#compute average word2vec for each test data query\n",
    "avg_w2v_vectors_test = [] \n",
    "for sentence in tqdm(x_test['Query'].values): \n",
    "    vector = np.zeros(300) \n",
    "    cnt_words =0\n",
    "    for word in sentence.split(): \n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors_test.append(vector)\n",
    "\n",
    "print(len(avg_w2v_vectors_test))\n",
    "print(len(avg_w2v_vectors_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.8.5 Featurizing text data or Given SQL Queries using Tfidf weighted word2vec </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_model.fit(x_train['Query'].values)\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n",
    "tfidf_words = set(tfidf_model.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21523/21523 [00:00<00:00, 24452.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21523\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#computing tfidf word2vec for each train data query\n",
    "tfidf_w2v_vectors_train = []\n",
    "for sentence in tqdm(x_train['Query'].values):\n",
    "    vector = np.zeros(300)\n",
    "    tf_idf_weight =0\n",
    "    for word in sentence.split(): \n",
    "        if (word in glove_words) and (word in tfidf_words):\n",
    "            vec = model[word] \n",
    "           \n",
    "            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) \n",
    "            vector += (vec * tf_idf)\n",
    "            tf_idf_weight += tf_idf\n",
    "    if tf_idf_weight != 0:\n",
    "        vector /= tf_idf_weight\n",
    "    tfidf_w2v_vectors_train.append(vector)\n",
    "\n",
    "print(len(tfidf_w2v_vectors_train))\n",
    "print(len(tfidf_w2v_vectors_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9225/9225 [00:00<00:00, 25488.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9225\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#computing tfidf word2vec for each test data query\n",
    "tfidf_w2v_vectors_test = []\n",
    "for sentence in tqdm(x_test['Query'].values):\n",
    "    vector = np.zeros(300)\n",
    "    tf_idf_weight =0\n",
    "    for word in sentence.split(): \n",
    "        if (word in glove_words) and (word in tfidf_words):\n",
    "            vec = model[word] \n",
    "           \n",
    "            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) \n",
    "            vector += (vec * tf_idf)\n",
    "            tf_idf_weight += tf_idf\n",
    "    if tf_idf_weight != 0:\n",
    "        vector /= tf_idf_weight\n",
    "    tfidf_w2v_vectors_test.append(vector)\n",
    "\n",
    "print(len(tfidf_w2v_vectors_test))\n",
    "print(len(tfidf_w2v_vectors_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9 Encoding Numerical features </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.1 Encoding Numerical feature : number of single line quotation marks </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_single_qts after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Since we have numerical features will transform the numerical features using standardscaler method\n",
    "\n",
    "'''\n",
    "#Generalized function for standardscaler method\n",
    "def transform(x_train,x_test):\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(x_train.reshape(-1,1))\n",
    "    train_d = sc.transform(x_train.reshape(-1,1))\n",
    "    test_d = sc.transform(x_test.reshape(-1,1))\n",
    "    return train_d,test_d\n",
    "\n",
    "\n",
    "train_no_single_qts,test_no_single_qts  = transform(np.array(x_train['no_single_qts']).reshape(-1,1),np.array(x_test['no_single_qts']).reshape(-1,1))\n",
    "\n",
    "\n",
    "print(\"the shape of no_single_qts after Encoding is {}\".format(train_no_single_qts.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.2 Encoding Numerical feature : number of double line quotation marks </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_double_qts after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_no_double_qts,test_no_double_qts  = transform(np.array(x_train['no_double_qts']).reshape(-1,1),np.array(x_test['no_double_qts']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of no_double_qts after Encoding is {}\".format(train_no_double_qts.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.3 Encoding Numerical feature : number of punctuations </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_punct after Encoding is (9225, 1)\n"
     ]
    }
   ],
   "source": [
    "#alpha value is choosen as 1 because if there are no elements present then denominator will be zero\n",
    "\n",
    "train_no_punct,test_no_punct  = transform(np.array(x_train['no_punct']).reshape(-1,1),np.array(x_test['no_punct']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of no_punct after Encoding is {}\".format(test_no_punct.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.4 Encoding Numerical feature : number of single line comments </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_single_cmnt after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_no_single_cmnt,test_no_single_cmnt  = transform(np.array(x_train['no_single_cmnt']).reshape(-1,1),np.array(x_test['no_single_cmnt']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of no_single_cmnt after Encoding is {}\".format(train_no_single_cmnt.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.5 Encoding Numerical feature : number of multi line comments </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_mult_cmnt after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_no_mult_cmnt,test_no_mult_cmnt = transform(np.array(x_train['no_mult_cmnt']).reshape(-1,1),np.array(x_test['no_mult_cmnt']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of no_mult_cmnt after Encoding is {}\".format(train_no_mult_cmnt.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.6 Encoding Numerical feature : number of space </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_space after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_no_space,test_no_space  = transform(np.array(x_train['no_space']).reshape(-1,1),np.array(x_test['no_space']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of no_space after Encoding is {}\".format(train_no_space.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.7 Encoding Numerical feature : number of percentage symbols </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_perc after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_no_perc,test_no_perc  = transform(np.array(x_train['no_perc']).reshape(-1,1),np.array(x_test['no_perc']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of no_perc after Encoding is {}\".format(train_no_perc.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.8 Encoding Numerical feature : number of logical operators </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_log_opt after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_no_log_opt,test_no_log_opt  = transform(np.array(x_train['no_log_opt']).reshape(-1,1),np.array(x_test['no_log_opt']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of no_log_opt after Encoding is {}\".format(train_no_log_opt.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.9 Encoding Numerical feature : number of arithmetic operators </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_arith_opt after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_no_arith_opt,test_no_arith_opt  = transform(np.array(x_train['no_arith']).reshape(-1,1),np.array(x_test['no_arith']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of no_arith_opt after Encoding is {}\".format(train_no_arith_opt.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.10 Encoding Numerical feature : number of null values </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_null after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_no_null,test_no_null  = transform(np.array(x_train['no_null']).reshape(-1,1),np.array(x_test['no_null']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of no_null after Encoding is {}\".format(train_no_null.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.10 Encoding Numerical feature : number of hexadecimal values </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_hexa after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_no_hexa,test_no_hexa = transform(np.array(x_train['no_hexa']).reshape(-1,1),np.array(x_test['no_hexa']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of no_hexa after Encoding is {}\".format(train_no_hexa.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.11 Encoding Numerical feature : number of alphabets </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_alpha after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_no_alpha,test_no_alpha = transform(np.array(x_train['no_alpha']).reshape(-1,1),np.array(x_test['no_alpha']).reshape(-1,1)) \n",
    "\n",
    "print(\"the shape of no_alpha after Encoding is {}\".format(train_no_alpha.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.12 Encoding Numerical feature : number of digits </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of no_digit after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_no_digit,test_no_digit = transform(np.array(x_train['no_digit']).reshape(-1,1),np.array(x_test['no_digit']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of no_digit after Encoding is {}\".format(train_no_digit.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.13 Encoding Numerical feature : length of che,char and null keywords combined </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of len_of_chr_char_null after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_len_of_chr_char_null,test_len_of_chr_char_null  = transform(np.array(x_train['len_of_chr_char_null']).reshape(-1,1),np.array(x_test['len_of_chr_char_null']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of len_of_chr_char_null after Encoding is {}\".format(train_len_of_chr_char_null.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.9.14 Encoding Numerical feature : length of genuine_keywords </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of genuine_keywords after Encoding is (21523, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_genuine_keywords,test_genuine_keywords  = transform(np.array(x_train['genuine_keywords']).reshape(-1,1),np.array(x_test['genuine_keywords']).reshape(-1,1))\n",
    "\n",
    "print(\"the shape of genuine_keywords after Encoding is {}\".format(train_genuine_keywords.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Combining all the features encoded above</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.1 Concatenating Encoded features with bag of words unigram range encoding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bow_unigram = hstack((x_train_bow_unigram,train_no_single_qts,train_no_double_qts,train_no_punct,train_no_single_cmnt,train_no_mult_cmnt,train_no_space,train_no_perc,train_no_log_opt,train_no_arith_opt,train_no_null,train_no_hexa,train_no_alpha,train_no_digit,train_len_of_chr_char_null,train_genuine_keywords)).tocsr()\n",
    "test_data_bow_unigram = hstack((x_test_bow_unigram,test_no_single_qts,test_no_double_qts,test_no_punct,test_no_single_cmnt,test_no_mult_cmnt,test_no_space,test_no_perc,test_no_log_opt,test_no_arith_opt,test_no_null,test_no_hexa,test_no_alpha,test_no_digit,test_len_of_chr_char_null,test_genuine_keywords)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.2 Concatenating Encoded features with bag of words bigram range encoding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bow_bigram = hstack((x_train_bow_bigram,train_no_single_qts,train_no_double_qts,train_no_punct,train_no_single_cmnt,train_no_mult_cmnt,train_no_space,train_no_perc,train_no_log_opt,train_no_arith_opt,train_no_null,train_no_hexa,train_no_alpha,train_no_digit,train_len_of_chr_char_null,train_genuine_keywords)).tocsr()\n",
    "test_data_bow_bigram = hstack((x_test_bow_bigram,test_no_single_qts,test_no_double_qts,test_no_punct,test_no_single_cmnt,test_no_mult_cmnt,test_no_space,test_no_perc,test_no_log_opt,test_no_arith_opt,test_no_null,test_no_hexa,test_no_alpha,test_no_digit,test_len_of_chr_char_null,test_genuine_keywords)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.3 Concatenating Encoded features with tfidf vectorizer unigram encoding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tfidf_unigram = hstack((x_train_tfidf_unigram,train_no_single_qts,train_no_double_qts,train_no_punct,train_no_single_cmnt,train_no_mult_cmnt,train_no_space,train_no_perc,train_no_log_opt,train_no_arith_opt,train_no_null,train_no_hexa,train_no_alpha,train_no_digit,train_len_of_chr_char_null,train_genuine_keywords)).tocsr()\n",
    "test_data_tfidf_unigram = hstack((x_test_tfidf_unigram,test_no_single_qts,test_no_double_qts,test_no_punct,test_no_single_cmnt,test_no_mult_cmnt,test_no_space,test_no_perc,test_no_log_opt,test_no_arith_opt,test_no_null,test_no_hexa,test_no_alpha,test_no_digit,test_len_of_chr_char_null,test_genuine_keywords)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.4 Concatenating Encoded features with tfidf vectorizer bigram encoding</h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tfidf_bigram = hstack((x_train_tfidf_bigram,train_no_single_qts,train_no_double_qts,train_no_punct,train_no_single_cmnt,train_no_mult_cmnt,train_no_space,train_no_perc,train_no_log_opt,train_no_arith_opt,train_no_null,train_no_hexa,train_no_alpha,train_no_digit,train_len_of_chr_char_null,train_genuine_keywords)).tocsr()\n",
    "test_data_tfidf_bigram = hstack((x_test_tfidf_bigram,test_no_single_qts,test_no_double_qts,test_no_punct,test_no_single_cmnt,test_no_mult_cmnt,test_no_space,test_no_perc,test_no_log_opt,test_no_arith_opt,test_no_null,test_no_hexa,test_no_alpha,test_no_digit,test_len_of_chr_char_null,test_genuine_keywords)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.5 Concatenating Encoded features with Average word2vec</h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_avg_word2vec = np.hstack((np.array(avg_w2v_vectors_train),train_no_single_qts,train_no_double_qts,train_no_punct,train_no_single_cmnt,train_no_mult_cmnt,train_no_space,train_no_perc,train_no_log_opt,train_no_arith_opt,train_no_null,train_no_hexa,train_no_alpha,train_no_digit,train_len_of_chr_char_null,train_genuine_keywords))\n",
    "test_data_avg_word2vec = np.hstack((np.array(avg_w2v_vectors_test),test_no_single_qts,test_no_double_qts,test_no_punct,test_no_single_cmnt,test_no_mult_cmnt,test_no_space,test_no_perc,test_no_log_opt,test_no_arith_opt,test_no_null,test_no_hexa,test_no_alpha,test_no_digit,test_len_of_chr_char_null,test_genuine_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.6 Concatenating Encoded features with tfidf word2vec</h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tfidf_word2vec = np.hstack((np.array(tfidf_w2v_vectors_train),train_no_single_qts,train_no_double_qts,train_no_punct,train_no_single_cmnt,train_no_mult_cmnt,train_no_space,train_no_perc,train_no_log_opt,train_no_arith_opt,train_no_null,train_no_hexa,train_no_alpha,train_no_digit,train_len_of_chr_char_null,train_genuine_keywords))\n",
    "test_data_tfidf_word2vec = np.hstack((np.array(tfidf_w2v_vectors_test),test_no_single_qts,test_no_double_qts,test_no_punct,test_no_single_cmnt,test_no_mult_cmnt,test_no_space,test_no_perc,test_no_log_opt,test_no_arith_opt,test_no_null,test_no_hexa,test_no_alpha,test_no_digit,test_len_of_chr_char_null,test_genuine_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing all features generated above for further use\n",
    "sparse.save_npz(\"train_data_bow_unigram\",train_data_bow_unigram)\n",
    "sparse.save_npz(\"test_data_bow_bigram.npz\",test_data_bow_bigram)\n",
    "sparse.save_npz(\"train_data_tfidf_unigram.npz\",train_data_tfidf_unigram)\n",
    "sparse.save_npz(\"train_data_tfidf_bigram.npz\",train_data_tfidf_bigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(\"test_data_bow_unigram\",test_data_bow_unigram)\n",
    "sparse.save_npz(\"train_data_bow_bigram.npz\",train_data_bow_bigram)\n",
    "sparse.save_npz(\"test_data_tfidf_unigram.npz\",test_data_tfidf_unigram)\n",
    "sparse.save_npz(\"test_data_tfidf_bigram.npz\",test_data_tfidf_bigram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.to_csv('x_train.csv')\n",
    "x_test.to_csv('x_test.csv')\n",
    "y_train.to_csv('y_train.csv')\n",
    "y_test.to_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pickle files to store word2vec numpy arrays\n",
    "with open('train_data_avg_word2vec','wb') as f:\n",
    "    pickle.dump(train_data_avg_word2vec,f)\n",
    "    \n",
    "with open('test_data_avg_word2vec','wb') as f:\n",
    "    pickle.dump(test_data_avg_word2vec,f)\n",
    "    \n",
    "with open('train_data_tfidf_word2vec','wb') as f:\n",
    "    pickle.dump(train_data_tfidf_word2vec,f)\n",
    "    \n",
    "with open('test_data_tfidf_word2vec','wb') as f:\n",
    "    pickle.dump(test_data_tfidf_word2vec,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
